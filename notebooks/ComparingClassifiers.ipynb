{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Outline:\n",
    "\n",
    "  * **Data Extraction: Read CSV data and remove/fill-in missing values**\n",
    "  * **Feature Extraction: Categorize/Encode features and remove unwanted/ redundant features.**\n",
    "  * **Pipeline: Pipe the transformation and predictors/classifiers**\n",
    "  * **Evaluate Classifiers/Predictors: Cross val the pipeline using appropriate scoring metric**\n",
    "  * **Fine Tuning: Searching over Parameters using GridSearchCV**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline, make_pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ** Sklearn doesn't have a way to choose particular feature to be used for transformation in the pipeline. Thus we require our own Function Transformer to handle column selection.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "class ItemSelector(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"For data grouped by feature, select subset of data at a provided key.\n",
    "\n",
    "    The data is expected to be stored in a 2D data structure, where the first\n",
    "    index is over features and the second is over samples.  i.e.\n",
    "\n",
    "    >> len(data[key]) == n_samples\n",
    "\n",
    "    Please note that this is the opposite convention to sklearn feature\n",
    "    matrixes (where the first index corresponds to sample).\n",
    "\n",
    "    ItemSelector only requires that the collection implement getitem\n",
    "    (data[key]).  Examples include: a dict of lists, 2D numpy array, Pandas\n",
    "    DataFrame, numpy record array, etc.\n",
    "\n",
    "    >> data = {'a': [1, 5, 2, 5, 2, 8],\n",
    "               'b': [9, 4, 1, 4, 1, 3]}\n",
    "    >> ds = ItemSelector(key='a')\n",
    "    >> data['a'] == ds.transform(data)\n",
    "\n",
    "    ItemSelector is not designed to handle data grouped by sample.  (e.g. a\n",
    "    list of dicts).  If your data is structured this way, consider a\n",
    "    transformer along the lines of `sklearn.feature_extraction.DictVectorizer`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    key : hashable, required\n",
    "        The key corresponding to the desired value in a mappable.\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, x, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, data_dict):\n",
    "        return data_dict[self.key]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Extraction \n",
    "\n",
    "* ** Import data using pandas and perform cleaning.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is an example of nba shots data which we would be using to classify/predict a shot was made or not.\n",
    "data = pd.read_csv(\"../data/shot_logs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Data Cleaning -- Check for nulls and decide to drop them or fill them.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GAME_ID                       False\n",
       "MATCHUP                       False\n",
       "LOCATION                      False\n",
       "W                             False\n",
       "FINAL_MARGIN                  False\n",
       "SHOT_NUMBER                   False\n",
       "PERIOD                        False\n",
       "GAME_CLOCK                    False\n",
       "SHOT_CLOCK                     True\n",
       "DRIBBLES                      False\n",
       "TOUCH_TIME                    False\n",
       "SHOT_DIST                     False\n",
       "PTS_TYPE                      False\n",
       "SHOT_RESULT                   False\n",
       "CLOSEST_DEFENDER              False\n",
       "CLOSEST_DEFENDER_PLAYER_ID    False\n",
       "CLOSE_DEF_DIST                False\n",
       "FGM                           False\n",
       "PTS                           False\n",
       "player_name                   False\n",
       "player_id                     False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nulls in SHOT CLOCK feature 5567\n",
      "We see that number is small thus we can drop it.\n"
     ]
    }
   ],
   "source": [
    "# We see only shot_clock has null values\n",
    "# Let's check how many do we have and if we can drop it\n",
    "print \"Number of nulls in SHOT CLOCK feature\", data[data.SHOT_CLOCK.isnull()].shape[0]\n",
    "\n",
    "print \"We see that number is small thus we can drop it.\"\n",
    "dataForAnalysis = data.dropna().copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction\n",
    "* **Bucket the features or drop the unwanted/redundant features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Based on the knowledge of data, the below columns are unwanted for classification\n",
      "******************************\n",
      "Categorizing distance of the shot...\n",
      "******************************\n",
      "Categorizing the number of driibbles before the shot\n",
      "******************************\n",
      "Categorizing the defender distance before the shot\n",
      "******************************\n",
      "Encoding should be part of the pipeline but as Pipeline doesn't Label Encoding , we are doing it in the feature extraction\n"
     ]
    }
   ],
   "source": [
    "print '*'*30\n",
    "print \"Based on the knowledge of data, the below columns are unwanted for classification\"\n",
    "del dataForAnalysis[\"GAME_ID\"]  \n",
    "del dataForAnalysis['MATCHUP']\n",
    "del dataForAnalysis['GAME_CLOCK']\n",
    "del dataForAnalysis[\"FINAL_MARGIN\"]\n",
    "del dataForAnalysis[\"PTS\"]\n",
    "del dataForAnalysis[\"player_name\"]\n",
    "del dataForAnalysis[\"CLOSEST_DEFENDER\"]\n",
    "del dataForAnalysis[\"W\"] ### Match Result\n",
    "del dataForAnalysis['SHOT_RESULT']  ### Duplicate information , captured in FGM.\n",
    "del dataForAnalysis[\"player_id\"]\n",
    "del dataForAnalysis[\"CLOSEST_DEFENDER_PLAYER_ID\"]\n",
    "\n",
    "print '*'*30\n",
    "print \"Categorizing distance of the shot...\"\n",
    "dataForAnalysis[\"SHOT_DIST_CAT\"] = pd.cut(dataForAnalysis.SHOT_DIST, 7, labels = range(1,8))\n",
    "del dataForAnalysis[\"SHOT_DIST\"]\n",
    "\n",
    "print '*'*30\n",
    "print \"Categorizing the number of dribbles before the shot\"\n",
    "dataForAnalysis[\"DRIBBLES_CAT\"] = pd.cut(dataForAnalysis.DRIBBLES, 4, labels = range(1,5))\n",
    "del dataForAnalysis[\"DRIBBLES\"]\n",
    "\n",
    "print '*'*30\n",
    "print \"Categorizing the defender distance before the shot\"\n",
    "dataForAnalysis[\"CLOSE_DEF_DIST_CAT\"] = pd.cut(dataForAnalysis.CLOSE_DEF_DIST, 11, labels = range(1,12))\n",
    "del dataForAnalysis[\"CLOSE_DEF_DIST\"]\n",
    "print '*'*30\n",
    "\n",
    "print \"Encoding should be part of the pipeline but as Pipeline doesn't Label Encoding , we are doing it in the feature extraction\"\n",
    "le = LabelEncoder()\n",
    "dataForAnalysis[\"IS_HOME\"] = dataForAnalysis[[\"LOCATION\"]].apply(le.fit_transform)\n",
    "del dataForAnalysis[\"LOCATION\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "* **Funtion Transformers, Encoders and Classifiers.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cat_shot_clock(times, y= None):\n",
    "    \"\"\"\n",
    "    Custom Function Transformer for the time shot was made to convert into 3 categories\n",
    "    \"\"\"\n",
    "    rt = []\n",
    "    for time_a in times:\n",
    "        time = time_a[0]\n",
    "        if time > 0 and time < 9:\n",
    "            rt.append(0)\n",
    "        elif time >=9 and time < 17:\n",
    "            rt.append(1)\n",
    "        else:\n",
    "            rt.append(2)\n",
    "    return pd.DataFrame(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def touch_time_cat(touch_times, y=None):\n",
    "    \"\"\"\n",
    "    Custom Function Transformer for the touch-time before the shot was made to convert into 3 categories\n",
    "    \"\"\"\n",
    "    rt = []\n",
    "    for touch_time_a in touch_times:\n",
    "        touch_time = touch_time_a[0]\n",
    "        if touch_time <=2:\n",
    "            rt.append(0)\n",
    "        elif touch_time > 2 and touch_time <=6:\n",
    "            rt.append(1)\n",
    "        else:\n",
    "            rt.append(2)\n",
    "    return pd.DataFrame(rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Pipeling of all the transformation on the data before we hand it over to the classifer\n"
     ]
    }
   ],
   "source": [
    "print '*'*30\n",
    "print \"Pipeling of all the transformation on the data before we hand it over to the classifer\"\n",
    "binaryEncoder = ce.BinaryEncoder(cols = ['player_id', 'CLOSEST_DEFENDER_PLAYER_ID', 'SHOT_NUMBER'])\n",
    "pipeReadyForPrediction = FeatureUnion([\n",
    "    ('cat_shot_clock' , Pipeline([\n",
    "                       ('selector' , ItemSelector(key = ['SHOT_CLOCK'])),\n",
    "                        ('encoder'  , FunctionTransformer(cat_shot_clock))\n",
    "                        #('encoder'  , OneHotEncoder())\n",
    "                        ]))\n",
    "    ,    \n",
    "    ('touch_time_cat' , Pipeline([\n",
    "                       ('selector' , ItemSelector(key = ['TOUCH_TIME'])),\n",
    "                        ('encoder'  , FunctionTransformer(touch_time_cat))\n",
    "                       ]))\n",
    "        \n",
    "    , \n",
    "    ('shot_distance' , Pipeline([\n",
    "                       ('selector' , ItemSelector(key = ['SHOT_NUMBER'])),\n",
    "                        ('encoder'  , OneHotEncoder(sparse= False, handle_unknown='ignore'))\n",
    "                       ]))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******************************\n",
      "Plug in Logistic Classifer in the Pipeline\n",
      "******************************\n",
      "Plug in Random Forest Classifer in the Pipeline\n"
     ]
    }
   ],
   "source": [
    "print '*'*30\n",
    "print \"Plug in Logistic Classifer in the Pipeline\"\n",
    "logPipe = Pipeline([(\"tranformation\", pipeReadyForPrediction), (\"LogisticRegression\", LogisticRegression())])\n",
    "\n",
    "print '*'*30\n",
    "print \"Plug in Random Forest Classifer in the Pipeline\"\n",
    "randomForestPipe = Pipeline([(\"tranformation\", pipeReadyForPrediction), (\"RandomForest\", RandomForestClassifier(n_estimators=7))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Classifiers\n",
    "* **Cross Value Score for each of the pipeline.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Separate the features and the prediction\n",
      "Extracting Features...\n",
      "   SHOT_NUMBER  PERIOD  SHOT_CLOCK  TOUCH_TIME  PTS_TYPE SHOT_DIST_CAT  \\\n",
      "0            1       1        10.8         1.9         2             2   \n",
      "1            2       1         3.4         0.8         3             5   \n",
      "3            4       2        10.3         1.9         2             3   \n",
      "4            5       2        10.9         2.7         2             1   \n",
      "5            6       2         9.1         4.4         2             3   \n",
      "\n",
      "  DRIBBLES_CAT CLOSE_DEF_DIST_CAT  IS_HOME  \n",
      "0            1                  1        0  \n",
      "1            1                  2        0  \n",
      "3            1                  1        0  \n",
      "4            1                  1        0  \n",
      "5            1                  1        0  \n",
      "Prediction...\n",
      "0    1\n",
      "1    0\n",
      "3    0\n",
      "4    0\n",
      "5    0\n",
      "Name: FGM, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print \"Separate the features and the prediction\"\n",
    "\n",
    "print \"Extracting Features...\"\n",
    "features_X = dataForAnalysis.ix[:, dataForAnalysis.columns != 'FGM']  \n",
    "print features_X.head()\n",
    "print \"Prediction...\"\n",
    "predict_Y = dataForAnalysis.FGM\n",
    "print predict_Y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average roc_auc score across 7 folds of LogisticRegression is 0.552466983055 \n",
      "******************************\n",
      "Average roc_auc score across 7 folds of RandomForest is 0.549155766368 \n",
      "******************************\n"
     ]
    }
   ],
   "source": [
    "folds = 7\n",
    "scoring_metric = 'roc_auc'\n",
    "for pipe in [logPipe, randomForestPipe]:\n",
    "    mean_score = np.mean(cross_val_score(pipe, features_X, predict_Y, cv = folds, scoring = scoring_metric))\n",
    "    print \"Average {0} score across {1} folds of {2} is {3} \".format(scoring_metric, folds, pipe.steps[-1][0], mean_score)\n",
    "    print '*'*30\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "* **Fine tuning C (Logistic Regression), Estimators (Random Forest).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search is good for fine tuning and finding the best estimator but it's computationally expensive\n"
     ]
    }
   ],
   "source": [
    "print \"Grid Search is good for fine tuning and finding the best estimator but it's computationally expensive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best cross-validated accuracy:  0.552942755629\n",
      "Best parameter found:  {'LogisticRegression__C': 0.001}\n",
      "Fitted_model:  LogisticRegression(C=0.001, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Best cross-validated accuracy:  0.549745372766\n",
      "Best parameter found:  {'RandomForest__n_estimators': 88}\n",
      "Fitted_model:  RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=88, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "C = [0.001, 0.1, 10]\n",
    "estimators_range = [11,31,88]\n",
    "cv_folds = 37\n",
    "log_param_grid = dict(LogisticRegression__C = C)\n",
    "rf_param_grid = dict(RandomForest__n_estimators = estimators_range)\n",
    "for pipe, params in [(logPipe, log_param_grid), (randomForestPipe, rf_param_grid)]:\n",
    "    grid = GridSearchCV(pipe, params, cv = cv_folds, scoring = scoring_metric, n_jobs=-1)\n",
    "    grid.fit(features_X, predict_Y)\n",
    "    print \"Best cross-validated auc: \",grid.best_score_\n",
    "    print \"Best parameter found: \",grid.best_params_\n",
    "    print \"Fitted_model: \",grid.best_estimator_.steps[1][1]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
